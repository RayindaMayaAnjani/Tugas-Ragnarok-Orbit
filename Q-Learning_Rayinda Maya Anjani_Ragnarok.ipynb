{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install gym\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah # 1 : Mengimpor libraby yang diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from windy_gridworld import WindyGridworldEnv\n",
    "import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah #2 : Membuat gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah #3 : Make the -greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based\n",
    "    on a given Q-function and epsilon.\n",
    "    Returns a function that takes the state\n",
    "    as an input and returns the probabilities\n",
    "    for each action in the form of a numpy array\n",
    "    of length of the action space(set of possible actions).\n",
    "    \"\"\"\n",
    "    def policyFunction(state):\n",
    "        Action_probabilities = np.ones(num_actions,\n",
    "                                       dtype = float) * epsilon / num_actions\n",
    "        best_action = np.argmax(Q[state])\n",
    "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
    "        return Action_probabilities\n",
    "    return policyFunction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah #4 : Bangun Model Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "              alpha = 0.6, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control.\n",
    "    Finds the optimal greedy policy while improving\n",
    "    following an epsilon-greedy policy\"\"\"\n",
    "    # Action value function\n",
    "    # A nested dictionary that maps\n",
    "    # state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths = np.zeros(num_episodes),\n",
    "        episode_rewards = np.zeros(num_episodes))\n",
    "    # Create an epsilon greedy policy function\n",
    "    # appropriately for environment action space\n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
    "    # For every episode\n",
    "    for ith_episode in range(num_episodes):\n",
    "    \n",
    "    # Reset the environment and pick the first action\n",
    "    state = env.reset() \n",
    "    for t in itertools.count():\n",
    "        # get probabilities of all actions from current state\n",
    "        action_probabilities = policy(state)\n",
    "\n",
    "        # choose action according to\n",
    "        # the probability distribution\n",
    "        action = np.random.choice(np.arange(\n",
    "            len(action_probabilities)),\n",
    "                                  p = action_probabilities)\n",
    "\n",
    "        # take action and get reward, transit to next state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # Update statistics\n",
    "        stats.episode_rewards[ith_episode] += reward\n",
    "        stats.episode_lengths[ith_episode] = t\n",
    "\n",
    "        # TD Update\n",
    "        best_next_action = np.argmax(Q[next_state])\n",
    "        td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "        td_delta = td_target - Q[state][action]\n",
    "        Q[state][action] += alpha * td_delta\n",
    "\n",
    "        # done is True if episode terminated\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            state = next_state\n",
    "            return Q, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah #5 : Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q, stats = qLearning(env, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah #6 : Plot important statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting.plot_episode_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kesimpulan:\n",
    "Disini terlihat bahwa dalam plot Episode Reward over time plot, episode reward semakin meningkat \n",
    "dari waktu ke waktu dan akhirnya levels out pada nilai hadiah per episode yang tinggi menunjukkan \n",
    "bahwa agen telah learnt untuk memaksimalkan total reward yang diperoleh dalam sebuah episode dengan \n",
    "berperilaku optimal di setiap statenya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
